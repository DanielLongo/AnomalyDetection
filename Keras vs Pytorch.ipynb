{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sin_wave(length, freq):\n",
    "    t = np.arange(length)\n",
    "    x = np.sin(2*np.pi * (1/freq) * t)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(300):\n",
    "    example = (get_sin_wave(length=260, freq=random.randint(32, 64)).reshape(1, -1) + 2)/2\n",
    "#     example = example.T\n",
    "    start = 0\n",
    "    end = start + 250\n",
    "    pred_end = end + 10\n",
    "    X_train.append(example[:, start:end])\n",
    "    y_train.append(example[:, end:pred_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2765 recordings\n",
      "By set limit only using 300 recordings\n",
      "Removing recordings of insufficient length...\n",
      "Removed 0 of 300 recordings. There are now 300 recordings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-258:\n",
      "Process ForkPoolWorker-260:\n",
      "Process ForkPoolWorker-261:\n",
      "Process ForkPoolWorker-263:\n",
      "Process ForkPoolWorker-259:\n",
      "Process ForkPoolWorker-262:\n",
      "Process ForkPoolWorker-256:\n",
      "Process ForkPoolWorker-267:\n",
      "Process ForkPoolWorker-266:\n",
      "Process ForkPoolWorker-257:\n",
      "Process ForkPoolWorker-264:\n",
      "Process ForkPoolWorker-265:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/mnt/home2/dlongo/.conda/envs/mne-4/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "train_dataset_csv = settings.TRAIN_DATASET_CSV\n",
    "dev_dataset_csv = settings.DEV_DATASET_CSV\n",
    "real_train_dataset = EEGDataset(train_dataset_csv, 260, [1], max_num_examples=300, transform=normalize)\n",
    "\n",
    "X_real_train = []\n",
    "y_real_train = []\n",
    "for i in range(300):\n",
    "    example = real_train_dataset[i].numpy()\n",
    "#     example = example.T\n",
    "    start = 0\n",
    "    end = start + 250\n",
    "    pred_end = end + 10\n",
    "    X_real_train.append(example[:, start:end])\n",
    "    y_real_train.append(example[:, end:pred_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_real_train)\n",
    "y_train = np.asarray(y_real_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchLSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=80, num_layers=2, output_size=1, n_predictions=10, dropout=.3):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_predictions = n_predictions\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_layer_size, num_layers=1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(p=.3)\n",
    "        self.lstm2 = nn.LSTM(hidden_layer_size, hidden_layer_size, num_layers=1, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(p=.3)\n",
    "#         self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(hidden_layer_size, n_predictions * output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        out, (_, _) = self.lstm1(input_seq)\n",
    "        out = self.dropout1(out)\n",
    "        out, (_, _) = self.lstm2(out)\n",
    "        out = self.dropout2(out)\n",
    "        preds = self.fc1(out[:,-1].squeeze()).squeeze()\n",
    "        return preds\n",
    "#         return self.relu(preds)\n",
    "#     def forward(self, input_seq):\n",
    "#         out, (h_n,_) = self.lstm(input_seq)\n",
    "#         h_n = h_n[-1] # from last layer\n",
    "#         h_n = h_n.squeeze()\n",
    "#         preds = self.fc1(h_n).view(-1, self.n_predictions, self.output_size)\n",
    "#         preds = self.fc1(out[:,-1,:].squeeze()).squeeze()\n",
    "#         return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "things to check\n",
    "- initialization \n",
    "- check outputs vs keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    torch_model.train()\n",
    "    running_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            a = torch_model(x.transpose(1,2))\n",
    "            loss = criteron(a.squeeze(), y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_losses.append(loss.item())\n",
    "            if i % 10 == 0:\n",
    "                cur_iter = len(train_loader) * epoch + i\n",
    "                print(f\"epoch {epoch} | iter {cur_iter} | Running Loss {sum(running_losses)/len(running_losses)}\")\n",
    "                running_losses.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TorchLSTM' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-522-396ad55b6d20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/mne-4/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 535\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TorchLSTM' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "torch_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = TorchLSTM(n_predictions=10, hidden_layer_size=80, num_layers=2)\n",
    "criteron = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(torch_model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | iter 0 | Running Loss 0.3154061734676361\n",
      "epoch 1 | iter 5 | Running Loss 0.2690798431634903\n",
      "epoch 2 | iter 10 | Running Loss 0.19070129096508026\n",
      "epoch 3 | iter 15 | Running Loss 0.11907382905483246\n",
      "epoch 4 | iter 20 | Running Loss 0.09414765685796737\n",
      "epoch 5 | iter 25 | Running Loss 0.0832745149731636\n",
      "epoch 6 | iter 30 | Running Loss 0.08106383085250854\n",
      "epoch 7 | iter 35 | Running Loss 0.06573965251445771\n",
      "epoch 8 | iter 40 | Running Loss 0.049335597455501555\n",
      "epoch 9 | iter 45 | Running Loss 0.04430879727005958\n",
      "epoch 10 | iter 50 | Running Loss 0.04627854749560356\n",
      "epoch 11 | iter 55 | Running Loss 0.039071603864431384\n",
      "epoch 12 | iter 60 | Running Loss 0.03867567703127861\n",
      "epoch 13 | iter 65 | Running Loss 0.034839557483792305\n",
      "epoch 14 | iter 70 | Running Loss 0.03724113777279854\n",
      "epoch 15 | iter 75 | Running Loss 0.03327190652489662\n",
      "epoch 16 | iter 80 | Running Loss 0.0336956575512886\n",
      "epoch 17 | iter 85 | Running Loss 0.03290397301316261\n",
      "epoch 18 | iter 90 | Running Loss 0.03203193955123425\n",
      "epoch 19 | iter 95 | Running Loss 0.03256187327206135\n",
      "epoch 20 | iter 100 | Running Loss 0.033293678984045984\n",
      "epoch 21 | iter 105 | Running Loss 0.03166655674576759\n",
      "epoch 22 | iter 110 | Running Loss 0.031898997724056244\n",
      "epoch 23 | iter 115 | Running Loss 0.03004610352218151\n",
      "epoch 24 | iter 120 | Running Loss 0.02791113518178463\n",
      "epoch 25 | iter 125 | Running Loss 0.027961299940943717\n",
      "epoch 26 | iter 130 | Running Loss 0.03128633312880993\n",
      "epoch 27 | iter 135 | Running Loss 0.02767898514866829\n",
      "epoch 28 | iter 140 | Running Loss 0.030467558279633523\n",
      "epoch 29 | iter 145 | Running Loss 0.028821059316396714\n",
      "epoch 30 | iter 150 | Running Loss 0.025241613760590552\n",
      "epoch 31 | iter 155 | Running Loss 0.029398874565958976\n",
      "epoch 32 | iter 160 | Running Loss 0.025662929750978947\n",
      "epoch 33 | iter 165 | Running Loss 0.026620523259043693\n",
      "epoch 34 | iter 170 | Running Loss 0.024634725600481033\n",
      "epoch 35 | iter 175 | Running Loss 0.02909064441919327\n",
      "epoch 36 | iter 180 | Running Loss 0.026732510700821877\n",
      "epoch 37 | iter 185 | Running Loss 0.02559628002345562\n",
      "epoch 38 | iter 190 | Running Loss 0.027603528648614883\n",
      "epoch 39 | iter 195 | Running Loss 0.02850845009088516\n",
      "epoch 40 | iter 200 | Running Loss 0.023264823481440544\n",
      "epoch 41 | iter 205 | Running Loss 0.02663094103336334\n",
      "epoch 42 | iter 210 | Running Loss 0.02663077488541603\n",
      "epoch 43 | iter 215 | Running Loss 0.02581281177699566\n",
      "epoch 44 | iter 220 | Running Loss 0.024995893612504004\n",
      "epoch 45 | iter 225 | Running Loss 0.02636324390769005\n",
      "epoch 46 | iter 230 | Running Loss 0.02793377675116062\n",
      "epoch 47 | iter 235 | Running Loss 0.02495105117559433\n",
      "epoch 48 | iter 240 | Running Loss 0.02705434039235115\n",
      "epoch 49 | iter 245 | Running Loss 0.025933587551116945\n",
      "epoch 50 | iter 250 | Running Loss 0.023899849504232407\n",
      "epoch 51 | iter 255 | Running Loss 0.02596191056072712\n",
      "epoch 52 | iter 260 | Running Loss 0.025584667176008224\n",
      "epoch 53 | iter 265 | Running Loss 0.02433876320719719\n",
      "epoch 54 | iter 270 | Running Loss 0.024277873709797858\n",
      "epoch 55 | iter 275 | Running Loss 0.02516794204711914\n",
      "epoch 56 | iter 280 | Running Loss 0.025849931687116624\n",
      "epoch 57 | iter 285 | Running Loss 0.02665698379278183\n",
      "epoch 58 | iter 290 | Running Loss 0.026986190304160117\n",
      "epoch 59 | iter 295 | Running Loss 0.02414095476269722\n",
      "epoch 60 | iter 300 | Running Loss 0.023820763267576695\n",
      "epoch 61 | iter 305 | Running Loss 0.028575045615434648\n",
      "epoch 62 | iter 310 | Running Loss 0.02478594593703747\n",
      "epoch 63 | iter 315 | Running Loss 0.026151740178465843\n",
      "epoch 64 | iter 320 | Running Loss 0.023940524086356163\n",
      "epoch 65 | iter 325 | Running Loss 0.02570110820233822\n",
      "epoch 66 | iter 330 | Running Loss 0.027797658741474152\n",
      "epoch 67 | iter 335 | Running Loss 0.027559948340058326\n",
      "epoch 68 | iter 340 | Running Loss 0.026723302155733108\n",
      "epoch 69 | iter 345 | Running Loss 0.02338155098259449\n",
      "epoch 70 | iter 350 | Running Loss 0.025207437574863434\n",
      "epoch 71 | iter 355 | Running Loss 0.024338005110621452\n",
      "epoch 72 | iter 360 | Running Loss 0.025670956820249557\n",
      "epoch 73 | iter 365 | Running Loss 0.026324987038969995\n",
      "epoch 74 | iter 370 | Running Loss 0.022269921749830245\n",
      "epoch 75 | iter 375 | Running Loss 0.027506206184625626\n",
      "epoch 76 | iter 380 | Running Loss 0.022312856651842593\n",
      "epoch 77 | iter 385 | Running Loss 0.02476028949022293\n",
      "epoch 78 | iter 390 | Running Loss 0.027234669029712676\n",
      "epoch 79 | iter 395 | Running Loss 0.02201504781842232\n",
      "epoch 80 | iter 400 | Running Loss 0.024276185408234598\n",
      "epoch 81 | iter 405 | Running Loss 0.02303556092083454\n",
      "epoch 82 | iter 410 | Running Loss 0.024542555399239063\n",
      "epoch 83 | iter 415 | Running Loss 0.023024535551667214\n",
      "epoch 84 | iter 420 | Running Loss 0.025355615094304083\n",
      "epoch 85 | iter 425 | Running Loss 0.02342837639153004\n",
      "epoch 86 | iter 430 | Running Loss 0.026425359584391118\n",
      "epoch 87 | iter 435 | Running Loss 0.02327697053551674\n",
      "epoch 88 | iter 440 | Running Loss 0.02500494346022606\n",
      "epoch 89 | iter 445 | Running Loss 0.024253382906317712\n",
      "epoch 90 | iter 450 | Running Loss 0.023026179522275925\n",
      "epoch 91 | iter 455 | Running Loss 0.023096559196710588\n",
      "epoch 92 | iter 460 | Running Loss 0.023669682815670966\n",
      "epoch 93 | iter 465 | Running Loss 0.025925399735569954\n",
      "epoch 94 | iter 470 | Running Loss 0.022175760380923747\n",
      "epoch 95 | iter 475 | Running Loss 0.024028995633125307\n",
      "epoch 96 | iter 480 | Running Loss 0.02175838127732277\n",
      "epoch 97 | iter 485 | Running Loss 0.02412322498857975\n",
      "epoch 98 | iter 490 | Running Loss 0.02451941668987274\n",
      "epoch 99 | iter 495 | Running Loss 0.023312779888510705\n",
      "epoch 100 | iter 500 | Running Loss 0.022871943190693855\n",
      "epoch 101 | iter 505 | Running Loss 0.026206453144550324\n",
      "epoch 102 | iter 510 | Running Loss 0.021144675835967065\n",
      "epoch 103 | iter 515 | Running Loss 0.023164601624011995\n",
      "epoch 104 | iter 520 | Running Loss 0.02406812347471714\n",
      "epoch 105 | iter 525 | Running Loss 0.02280256263911724\n",
      "epoch 106 | iter 530 | Running Loss 0.024121816083788872\n",
      "epoch 107 | iter 535 | Running Loss 0.02030692994594574\n",
      "epoch 108 | iter 540 | Running Loss 0.024289556592702866\n",
      "epoch 109 | iter 545 | Running Loss 0.022938346862792967\n",
      "epoch 110 | iter 550 | Running Loss 0.021752971410751342\n",
      "epoch 111 | iter 555 | Running Loss 0.021995193883776666\n",
      "epoch 112 | iter 560 | Running Loss 0.023713210970163344\n",
      "epoch 113 | iter 565 | Running Loss 0.021479495614767075\n",
      "epoch 114 | iter 570 | Running Loss 0.026035358756780626\n",
      "epoch 115 | iter 575 | Running Loss 0.023158034309744835\n",
      "epoch 116 | iter 580 | Running Loss 0.024248289689421654\n",
      "epoch 117 | iter 585 | Running Loss 0.022727727144956588\n",
      "epoch 118 | iter 590 | Running Loss 0.02278249654918909\n",
      "epoch 119 | iter 595 | Running Loss 0.024972494691610336\n",
      "epoch 120 | iter 600 | Running Loss 0.0214632336050272\n",
      "epoch 121 | iter 605 | Running Loss 0.023084401711821557\n",
      "epoch 122 | iter 610 | Running Loss 0.021747808158397674\n",
      "epoch 123 | iter 615 | Running Loss 0.02255644090473652\n",
      "epoch 124 | iter 620 | Running Loss 0.022154782339930533\n",
      "epoch 125 | iter 625 | Running Loss 0.023900165781378747\n",
      "epoch 126 | iter 630 | Running Loss 0.02242604233324528\n",
      "epoch 127 | iter 635 | Running Loss 0.020315836742520334\n",
      "epoch 128 | iter 640 | Running Loss 0.023740767501294613\n",
      "epoch 129 | iter 645 | Running Loss 0.023604920506477355\n",
      "epoch 130 | iter 650 | Running Loss 0.02194244638085365\n",
      "epoch 131 | iter 655 | Running Loss 0.021610412374138833\n",
      "epoch 132 | iter 660 | Running Loss 0.02303730174899101\n",
      "epoch 133 | iter 665 | Running Loss 0.025357209146022797\n",
      "epoch 134 | iter 670 | Running Loss 0.020233361050486565\n",
      "epoch 135 | iter 675 | Running Loss 0.021821825951337814\n",
      "epoch 136 | iter 680 | Running Loss 0.022453632205724716\n",
      "epoch 137 | iter 685 | Running Loss 0.024692393094301223\n",
      "epoch 138 | iter 690 | Running Loss 0.01982313524931669\n",
      "epoch 139 | iter 695 | Running Loss 0.02346755415201187\n",
      "epoch 140 | iter 700 | Running Loss 0.022172224894165992\n",
      "epoch 141 | iter 705 | Running Loss 0.023208091780543328\n",
      "epoch 142 | iter 710 | Running Loss 0.022491126880049707\n",
      "epoch 143 | iter 715 | Running Loss 0.021800485625863075\n",
      "epoch 144 | iter 720 | Running Loss 0.02208809107542038\n",
      "epoch 145 | iter 725 | Running Loss 0.02402076721191406\n",
      "epoch 146 | iter 730 | Running Loss 0.021537411957979202\n",
      "epoch 147 | iter 735 | Running Loss 0.02166159190237522\n",
      "epoch 148 | iter 740 | Running Loss 0.020289330184459685\n",
      "epoch 149 | iter 745 | Running Loss 0.02238764613866806\n",
      "epoch 150 | iter 750 | Running Loss 0.021309319883584976\n",
      "epoch 151 | iter 755 | Running Loss 0.022664340585470198\n",
      "epoch 152 | iter 760 | Running Loss 0.020369143038988114\n",
      "epoch 153 | iter 765 | Running Loss 0.023996897414326666\n",
      "epoch 154 | iter 770 | Running Loss 0.01941995993256569\n",
      "epoch 155 | iter 775 | Running Loss 0.022097107768058778\n",
      "epoch 156 | iter 780 | Running Loss 0.023521550372242926\n",
      "epoch 157 | iter 785 | Running Loss 0.02229217514395714\n",
      "epoch 158 | iter 790 | Running Loss 0.021575429290533066\n",
      "epoch 159 | iter 795 | Running Loss 0.020814086496829986\n",
      "epoch 160 | iter 800 | Running Loss 0.02465761862695217\n",
      "epoch 161 | iter 805 | Running Loss 0.020469022169709205\n",
      "epoch 162 | iter 810 | Running Loss 0.020677816495299338\n",
      "epoch 163 | iter 815 | Running Loss 0.022716105356812476\n",
      "epoch 164 | iter 820 | Running Loss 0.02045551985502243\n",
      "epoch 165 | iter 825 | Running Loss 0.021236196160316467\n",
      "epoch 166 | iter 830 | Running Loss 0.022121376916766168\n",
      "epoch 167 | iter 835 | Running Loss 0.020521924272179602\n",
      "epoch 168 | iter 840 | Running Loss 0.020517122372984885\n",
      "epoch 169 | iter 845 | Running Loss 0.021508092433214186\n",
      "epoch 170 | iter 850 | Running Loss 0.022102426365017892\n",
      "epoch 171 | iter 855 | Running Loss 0.023407227545976638\n",
      "epoch 172 | iter 860 | Running Loss 0.0193921085447073\n",
      "epoch 173 | iter 865 | Running Loss 0.022279920056462288\n",
      "epoch 174 | iter 870 | Running Loss 0.021001948788762093\n",
      "epoch 175 | iter 875 | Running Loss 0.02258944995701313\n",
      "epoch 176 | iter 880 | Running Loss 0.02251553162932396\n",
      "epoch 177 | iter 885 | Running Loss 0.020126085355877878\n",
      "epoch 178 | iter 890 | Running Loss 0.02157372757792473\n",
      "epoch 179 | iter 895 | Running Loss 0.021370966359972952\n",
      "epoch 180 | iter 900 | Running Loss 0.02282749451696873\n",
      "epoch 181 | iter 905 | Running Loss 0.02265723906457424\n",
      "epoch 182 | iter 910 | Running Loss 0.02155907228589058\n",
      "epoch 183 | iter 915 | Running Loss 0.020477576926350594\n",
      "epoch 184 | iter 920 | Running Loss 0.02221112810075283\n",
      "epoch 185 | iter 925 | Running Loss 0.02119242735207081\n",
      "epoch 186 | iter 930 | Running Loss 0.023598088696599007\n",
      "epoch 187 | iter 935 | Running Loss 0.02114657461643219\n",
      "epoch 188 | iter 940 | Running Loss 0.021610240265727044\n",
      "epoch 189 | iter 945 | Running Loss 0.022640983760356902\n",
      "epoch 190 | iter 950 | Running Loss 0.021363724395632742\n",
      "epoch 191 | iter 955 | Running Loss 0.021606509387493134\n",
      "epoch 192 | iter 960 | Running Loss 0.02010353971272707\n",
      "epoch 193 | iter 965 | Running Loss 0.022639723680913448\n",
      "epoch 194 | iter 970 | Running Loss 0.022523338347673415\n",
      "epoch 195 | iter 975 | Running Loss 0.02175525240600109\n",
      "epoch 196 | iter 980 | Running Loss 0.021856035105884075\n",
      "epoch 197 | iter 985 | Running Loss 0.023836694657802582\n",
      "epoch 198 | iter 990 | Running Loss 0.021619924902915956\n",
      "epoch 199 | iter 995 | Running Loss 0.020595281943678857\n",
      "epoch 200 | iter 1000 | Running Loss 0.02495782673358917\n",
      "epoch 201 | iter 1005 | Running Loss 0.019206240214407443\n",
      "epoch 202 | iter 1010 | Running Loss 0.020245054364204408\n",
      "epoch 203 | iter 1015 | Running Loss 0.02258320339024067\n",
      "epoch 204 | iter 1020 | Running Loss 0.021369751915335654\n",
      "epoch 205 | iter 1025 | Running Loss 0.021366189792752267\n",
      "epoch 206 | iter 1030 | Running Loss 0.01996022034436464\n",
      "epoch 207 | iter 1035 | Running Loss 0.02330612428486347\n",
      "epoch 208 | iter 1040 | Running Loss 0.01968465093523264\n",
      "epoch 209 | iter 1045 | Running Loss 0.02154924310743809\n",
      "epoch 210 | iter 1050 | Running Loss 0.019766270741820337\n",
      "epoch 211 | iter 1055 | Running Loss 0.023238666355609894\n",
      "epoch 212 | iter 1060 | Running Loss 0.021861986070871354\n",
      "epoch 213 | iter 1065 | Running Loss 0.02306382730603218\n",
      "epoch 214 | iter 1070 | Running Loss 0.019944827444851397\n",
      "epoch 215 | iter 1075 | Running Loss 0.020813577622175217\n",
      "epoch 216 | iter 1080 | Running Loss 0.022379590570926665\n",
      "epoch 217 | iter 1085 | Running Loss 0.018911854736506938\n",
      "epoch 218 | iter 1090 | Running Loss 0.023916588723659517\n",
      "epoch 219 | iter 1095 | Running Loss 0.02164357900619507\n",
      "epoch 220 | iter 1100 | Running Loss 0.020252824760973454\n",
      "epoch 221 | iter 1105 | Running Loss 0.020892642438411713\n",
      "epoch 222 | iter 1110 | Running Loss 0.018680106103420257\n",
      "epoch 223 | iter 1115 | Running Loss 0.02145306281745434\n",
      "epoch 224 | iter 1120 | Running Loss 0.021863188408315182\n",
      "epoch 225 | iter 1125 | Running Loss 0.02029167227447033\n",
      "epoch 226 | iter 1130 | Running Loss 0.019742926582694054\n",
      "epoch 227 | iter 1135 | Running Loss 0.021447406336665153\n",
      "epoch 228 | iter 1140 | Running Loss 0.021332059800624848\n",
      "epoch 229 | iter 1145 | Running Loss 0.021409536153078078\n",
      "epoch 230 | iter 1150 | Running Loss 0.020032587461173534\n",
      "epoch 231 | iter 1155 | Running Loss 0.020025291480123997\n",
      "epoch 232 | iter 1160 | Running Loss 0.021456970274448393\n",
      "epoch 233 | iter 1165 | Running Loss 0.0216394554823637\n",
      "epoch 234 | iter 1170 | Running Loss 0.020532215759158135\n",
      "epoch 235 | iter 1175 | Running Loss 0.022417698800563813\n",
      "epoch 236 | iter 1180 | Running Loss 0.021080012992024423\n",
      "epoch 237 | iter 1185 | Running Loss 0.021905186399817467\n",
      "epoch 238 | iter 1190 | Running Loss 0.020634057000279427\n",
      "epoch 239 | iter 1195 | Running Loss 0.019994309172034264\n",
      "epoch 240 | iter 1200 | Running Loss 0.021492299064993857\n",
      "epoch 241 | iter 1205 | Running Loss 0.022001726925373076\n",
      "epoch 242 | iter 1210 | Running Loss 0.021756136789917946\n",
      "epoch 243 | iter 1215 | Running Loss 0.02046184316277504\n",
      "epoch 244 | iter 1220 | Running Loss 0.022182729840278626\n",
      "epoch 245 | iter 1225 | Running Loss 0.02218097969889641\n",
      "epoch 246 | iter 1230 | Running Loss 0.021303322911262513\n",
      "epoch 247 | iter 1235 | Running Loss 0.023335639387369156\n",
      "epoch 248 | iter 1240 | Running Loss 0.019895414263010024\n",
      "epoch 249 | iter 1245 | Running Loss 0.020948491990566254\n",
      "epoch 250 | iter 1250 | Running Loss 0.02132211849093437\n",
      "epoch 251 | iter 1255 | Running Loss 0.02117333672940731\n",
      "epoch 252 | iter 1260 | Running Loss 0.021811559423804285\n",
      "epoch 253 | iter 1265 | Running Loss 0.020401469618082046\n",
      "epoch 254 | iter 1270 | Running Loss 0.02304009608924389\n",
      "epoch 255 | iter 1275 | Running Loss 0.01949386242777109\n",
      "epoch 256 | iter 1280 | Running Loss 0.019393564015626908\n",
      "epoch 257 | iter 1285 | Running Loss 0.02097792476415634\n",
      "epoch 258 | iter 1290 | Running Loss 0.019321588799357414\n",
      "epoch 259 | iter 1295 | Running Loss 0.021763347461819647\n",
      "epoch 260 | iter 1300 | Running Loss 0.020132473669946194\n",
      "epoch 261 | iter 1305 | Running Loss 0.022475807182490825\n",
      "epoch 262 | iter 1310 | Running Loss 0.01964277997612953\n",
      "epoch 263 | iter 1315 | Running Loss 0.022308674454689027\n",
      "epoch 264 | iter 1320 | Running Loss 0.02019638568162918\n",
      "epoch 265 | iter 1325 | Running Loss 0.023777106404304506\n",
      "epoch 266 | iter 1330 | Running Loss 0.01967708468437195\n",
      "epoch 267 | iter 1335 | Running Loss 0.02149769477546215\n",
      "epoch 268 | iter 1340 | Running Loss 0.022925716266036032\n",
      "epoch 269 | iter 1345 | Running Loss 0.01928862538188696\n",
      "epoch 270 | iter 1350 | Running Loss 0.023128338158130646\n",
      "epoch 271 | iter 1355 | Running Loss 0.02087036892771721\n",
      "epoch 272 | iter 1360 | Running Loss 0.021482843533158304\n",
      "epoch 273 | iter 1365 | Running Loss 0.021037360280752183\n",
      "epoch 274 | iter 1370 | Running Loss 0.02246707081794739\n",
      "epoch 275 | iter 1375 | Running Loss 0.020200594887137414\n",
      "epoch 276 | iter 1380 | Running Loss 0.020656756684184076\n",
      "epoch 277 | iter 1385 | Running Loss 0.022203465551137926\n",
      "epoch 278 | iter 1390 | Running Loss 0.019794274866580964\n",
      "epoch 279 | iter 1395 | Running Loss 0.02259610630571842\n",
      "epoch 280 | iter 1400 | Running Loss 0.021565493196249008\n",
      "epoch 281 | iter 1405 | Running Loss 0.019473267905414104\n",
      "epoch 282 | iter 1410 | Running Loss 0.02261287271976471\n",
      "epoch 283 | iter 1415 | Running Loss 0.019287693500518798\n",
      "epoch 284 | iter 1420 | Running Loss 0.022015654295682908\n",
      "epoch 285 | iter 1425 | Running Loss 0.020430710911750794\n",
      "epoch 286 | iter 1430 | Running Loss 0.02168103661388159\n",
      "epoch 287 | iter 1435 | Running Loss 0.01973312683403492\n",
      "epoch 288 | iter 1440 | Running Loss 0.02214062362909317\n",
      "epoch 289 | iter 1445 | Running Loss 0.021580726094543933\n",
      "epoch 290 | iter 1450 | Running Loss 0.02152022309601307\n",
      "epoch 291 | iter 1455 | Running Loss 0.023107895255088808\n",
      "epoch 292 | iter 1460 | Running Loss 0.018282542191445827\n",
      "epoch 293 | iter 1465 | Running Loss 0.021836574375629424\n",
      "epoch 294 | iter 1470 | Running Loss 0.021422208473086356\n",
      "epoch 295 | iter 1475 | Running Loss 0.02245420590043068\n",
      "epoch 296 | iter 1480 | Running Loss 0.018650108575820924\n",
      "epoch 297 | iter 1485 | Running Loss 0.020531496219336986\n",
      "epoch 298 | iter 1490 | Running Loss 0.022845929116010667\n",
      "epoch 299 | iter 1495 | Running Loss 0.020458508655428885\n",
      "epoch 300 | iter 1500 | Running Loss 0.02130318880081177\n",
      "epoch 301 | iter 1505 | Running Loss 0.020632242411375047\n",
      "epoch 302 | iter 1510 | Running Loss 0.020258036628365516\n",
      "epoch 303 | iter 1515 | Running Loss 0.02084609176963568\n",
      "epoch 304 | iter 1520 | Running Loss 0.02192748971283436\n",
      "epoch 305 | iter 1525 | Running Loss 0.020950556546449662\n",
      "epoch 306 | iter 1530 | Running Loss 0.02020557429641485\n",
      "epoch 307 | iter 1535 | Running Loss 0.020886331796646118\n",
      "epoch 308 | iter 1540 | Running Loss 0.02043312042951584\n",
      "epoch 309 | iter 1545 | Running Loss 0.018935033492743968\n",
      "epoch 310 | iter 1550 | Running Loss 0.021054309234023093\n",
      "epoch 311 | iter 1555 | Running Loss 0.020731846801936626\n",
      "epoch 312 | iter 1560 | Running Loss 0.022631246596574783\n",
      "epoch 313 | iter 1565 | Running Loss 0.01981477625668049\n",
      "epoch 314 | iter 1570 | Running Loss 0.020609404146671294\n",
      "epoch 315 | iter 1575 | Running Loss 0.020907440409064292\n",
      "epoch 316 | iter 1580 | Running Loss 0.02080348562449217\n",
      "epoch 317 | iter 1585 | Running Loss 0.02063152566552162\n",
      "epoch 318 | iter 1590 | Running Loss 0.020867924392223357\n",
      "epoch 319 | iter 1595 | Running Loss 0.022451888769865036\n",
      "epoch 320 | iter 1600 | Running Loss 0.021064252033829688\n",
      "epoch 321 | iter 1605 | Running Loss 0.01920083723962307\n",
      "epoch 322 | iter 1610 | Running Loss 0.02082411162555218\n",
      "epoch 323 | iter 1615 | Running Loss 0.0200515603646636\n",
      "epoch 324 | iter 1620 | Running Loss 0.021699778735637665\n",
      "epoch 325 | iter 1625 | Running Loss 0.020171309262514113\n",
      "epoch 326 | iter 1630 | Running Loss 0.02264474965631962\n",
      "epoch 327 | iter 1635 | Running Loss 0.018190354481339453\n",
      "epoch 328 | iter 1640 | Running Loss 0.020802946016192436\n",
      "epoch 329 | iter 1645 | Running Loss 0.019947943836450578\n",
      "epoch 330 | iter 1650 | Running Loss 0.020970804244279863\n",
      "epoch 331 | iter 1655 | Running Loss 0.020983507111668586\n",
      "epoch 332 | iter 1660 | Running Loss 0.019463669322431088\n",
      "epoch 333 | iter 1665 | Running Loss 0.02152462974190712\n",
      "epoch 334 | iter 1670 | Running Loss 0.018265913240611553\n",
      "epoch 335 | iter 1675 | Running Loss 0.020235825330018997\n",
      "epoch 336 | iter 1680 | Running Loss 0.021723823994398116\n",
      "epoch 337 | iter 1685 | Running Loss 0.020219599083065986\n",
      "epoch 338 | iter 1690 | Running Loss 0.020929419621825218\n",
      "epoch 339 | iter 1695 | Running Loss 0.019022619910538198\n",
      "epoch 340 | iter 1700 | Running Loss 0.019955390691757204\n",
      "epoch 341 | iter 1705 | Running Loss 0.020396277122199536\n",
      "epoch 342 | iter 1710 | Running Loss 0.02089710384607315\n",
      "epoch 343 | iter 1715 | Running Loss 0.020497144013643265\n",
      "epoch 344 | iter 1720 | Running Loss 0.02117750085890293\n",
      "epoch 345 | iter 1725 | Running Loss 0.019998204335570334\n",
      "epoch 346 | iter 1730 | Running Loss 0.02069082036614418\n",
      "epoch 347 | iter 1735 | Running Loss 0.021005867421627043\n",
      "epoch 348 | iter 1740 | Running Loss 0.019586663134396075\n",
      "epoch 349 | iter 1745 | Running Loss 0.02238072268664837\n",
      "epoch 350 | iter 1750 | Running Loss 0.020513343438506127\n",
      "epoch 351 | iter 1755 | Running Loss 0.020209528505802155\n",
      "epoch 352 | iter 1760 | Running Loss 0.020243420079350472\n",
      "epoch 353 | iter 1765 | Running Loss 0.021198807284235954\n",
      "epoch 354 | iter 1770 | Running Loss 0.01895914748311043\n",
      "epoch 355 | iter 1775 | Running Loss 0.01997240409255028\n",
      "epoch 356 | iter 1780 | Running Loss 0.02112436629831791\n",
      "epoch 357 | iter 1785 | Running Loss 0.019986193254590036\n",
      "epoch 358 | iter 1790 | Running Loss 0.020452835038304328\n",
      "epoch 359 | iter 1795 | Running Loss 0.01787143051624298\n",
      "epoch 360 | iter 1800 | Running Loss 0.021239405497908592\n",
      "epoch 361 | iter 1805 | Running Loss 0.01908048652112484\n",
      "epoch 362 | iter 1810 | Running Loss 0.021308482624590397\n",
      "epoch 363 | iter 1815 | Running Loss 0.017732257209718227\n",
      "epoch 364 | iter 1820 | Running Loss 0.021360262855887414\n",
      "epoch 365 | iter 1825 | Running Loss 0.019926971569657326\n",
      "epoch 366 | iter 1830 | Running Loss 0.02062213234603405\n",
      "epoch 367 | iter 1835 | Running Loss 0.02094845995306969\n",
      "epoch 368 | iter 1840 | Running Loss 0.01958269067108631\n",
      "epoch 369 | iter 1845 | Running Loss 0.022505995631217957\n",
      "epoch 370 | iter 1850 | Running Loss 0.020321527682244777\n",
      "epoch 371 | iter 1855 | Running Loss 0.019459190033376216\n",
      "epoch 372 | iter 1860 | Running Loss 0.02093128561973572\n",
      "epoch 373 | iter 1865 | Running Loss 0.018961444869637488\n",
      "epoch 374 | iter 1870 | Running Loss 0.020989808440208434\n",
      "epoch 375 | iter 1875 | Running Loss 0.01927127465605736\n",
      "epoch 376 | iter 1880 | Running Loss 0.01979471929371357\n",
      "epoch 377 | iter 1885 | Running Loss 0.019363962486386298\n",
      "epoch 378 | iter 1890 | Running Loss 0.020640680566430092\n",
      "epoch 379 | iter 1895 | Running Loss 0.01945156753063202\n",
      "epoch 380 | iter 1900 | Running Loss 0.019837835058569907\n",
      "epoch 381 | iter 1905 | Running Loss 0.021957284584641457\n",
      "epoch 382 | iter 1910 | Running Loss 0.01950250528752804\n",
      "epoch 383 | iter 1915 | Running Loss 0.019660063832998276\n",
      "epoch 384 | iter 1920 | Running Loss 0.019697750732302666\n",
      "epoch 385 | iter 1925 | Running Loss 0.021108340844511984\n",
      "epoch 386 | iter 1930 | Running Loss 0.017798152193427084\n",
      "epoch 387 | iter 1935 | Running Loss 0.02191995531320572\n",
      "epoch 388 | iter 1940 | Running Loss 0.017613399028778075\n",
      "epoch 389 | iter 1945 | Running Loss 0.021205918118357658\n",
      "epoch 390 | iter 1950 | Running Loss 0.02072785086929798\n",
      "epoch 391 | iter 1955 | Running Loss 0.020172841288149355\n",
      "epoch 392 | iter 1960 | Running Loss 0.021661242097616197\n",
      "epoch 393 | iter 1965 | Running Loss 0.020139236748218537\n",
      "epoch 394 | iter 1970 | Running Loss 0.01983793545514345\n",
      "epoch 395 | iter 1975 | Running Loss 0.01924782283604145\n",
      "epoch 396 | iter 1980 | Running Loss 0.022305257618427277\n",
      "epoch 397 | iter 1985 | Running Loss 0.018858814053237437\n",
      "epoch 398 | iter 1990 | Running Loss 0.01971983127295971\n",
      "epoch 399 | iter 1995 | Running Loss 0.020578449219465257\n",
      "epoch 400 | iter 2000 | Running Loss 0.020831554383039474\n",
      "epoch 401 | iter 2005 | Running Loss 0.01960800401866436\n",
      "epoch 402 | iter 2010 | Running Loss 0.017971163243055345\n",
      "epoch 403 | iter 2015 | Running Loss 0.02100910022854805\n",
      "epoch 404 | iter 2020 | Running Loss 0.01992960125207901\n",
      "epoch 405 | iter 2025 | Running Loss 0.019263778626918793\n",
      "epoch 406 | iter 2030 | Running Loss 0.020061011239886285\n",
      "epoch 407 | iter 2035 | Running Loss 0.019004246965050698\n",
      "epoch 408 | iter 2040 | Running Loss 0.021127031370997428\n",
      "epoch 409 | iter 2045 | Running Loss 0.01862731222063303\n",
      "epoch 410 | iter 2050 | Running Loss 0.020671549066901208\n",
      "epoch 411 | iter 2055 | Running Loss 0.02035969104617834\n",
      "epoch 412 | iter 2060 | Running Loss 0.019429270923137665\n",
      "epoch 413 | iter 2065 | Running Loss 0.019142570719122887\n",
      "epoch 414 | iter 2070 | Running Loss 0.019588495045900343\n",
      "epoch 415 | iter 2075 | Running Loss 0.01939398590475321\n",
      "epoch 416 | iter 2080 | Running Loss 0.01928212381899357\n",
      "epoch 417 | iter 2085 | Running Loss 0.020531808212399482\n",
      "epoch 418 | iter 2090 | Running Loss 0.020301823690533637\n",
      "epoch 419 | iter 2095 | Running Loss 0.01873109731823206\n",
      "epoch 420 | iter 2100 | Running Loss 0.02097846381366253\n",
      "epoch 421 | iter 2105 | Running Loss 0.019442505761981012\n",
      "epoch 422 | iter 2110 | Running Loss 0.02145965173840523\n",
      "epoch 423 | iter 2115 | Running Loss 0.019234617985785008\n",
      "epoch 424 | iter 2120 | Running Loss 0.018021556362509728\n",
      "epoch 425 | iter 2125 | Running Loss 0.017639197781682013\n",
      "epoch 426 | iter 2130 | Running Loss 0.020640411600470542\n",
      "epoch 427 | iter 2135 | Running Loss 0.01907845288515091\n",
      "epoch 428 | iter 2140 | Running Loss 0.01912241242825985\n",
      "epoch 429 | iter 2145 | Running Loss 0.020995338447391987\n",
      "epoch 430 | iter 2150 | Running Loss 0.01759737990796566\n",
      "epoch 431 | iter 2155 | Running Loss 0.021203967556357384\n",
      "epoch 432 | iter 2160 | Running Loss 0.018090042099356652\n",
      "epoch 433 | iter 2165 | Running Loss 0.01970394477248192\n",
      "epoch 434 | iter 2170 | Running Loss 0.02061930447816849\n",
      "epoch 435 | iter 2175 | Running Loss 0.02013191394507885\n",
      "epoch 436 | iter 2180 | Running Loss 0.01890435181558132\n",
      "epoch 437 | iter 2185 | Running Loss 0.020857586711645126\n",
      "epoch 438 | iter 2190 | Running Loss 0.018004272319376468\n",
      "epoch 439 | iter 2195 | Running Loss 0.02218143120408058\n",
      "epoch 440 | iter 2200 | Running Loss 0.01817635651677847\n",
      "epoch 441 | iter 2205 | Running Loss 0.01975560337305069\n",
      "epoch 442 | iter 2210 | Running Loss 0.019085510075092314\n",
      "epoch 443 | iter 2215 | Running Loss 0.021628885343670846\n",
      "epoch 444 | iter 2220 | Running Loss 0.019735369831323624\n",
      "epoch 445 | iter 2225 | Running Loss 0.01792468745261431\n",
      "epoch 446 | iter 2230 | Running Loss 0.019458050653338433\n",
      "epoch 447 | iter 2235 | Running Loss 0.01761639192700386\n",
      "epoch 448 | iter 2240 | Running Loss 0.019815212488174437\n",
      "epoch 449 | iter 2245 | Running Loss 0.017867774702608585\n",
      "epoch 450 | iter 2250 | Running Loss 0.020789000391960143\n",
      "epoch 451 | iter 2255 | Running Loss 0.017975778505206107\n",
      "epoch 452 | iter 2260 | Running Loss 0.019561833515763283\n",
      "epoch 453 | iter 2265 | Running Loss 0.01905797142535448\n",
      "epoch 454 | iter 2270 | Running Loss 0.019414228200912476\n",
      "epoch 455 | iter 2275 | Running Loss 0.01814383938908577\n",
      "epoch 456 | iter 2280 | Running Loss 0.019580461084842682\n",
      "epoch 457 | iter 2285 | Running Loss 0.017407978326082228\n",
      "epoch 458 | iter 2290 | Running Loss 0.020441267639398575\n",
      "epoch 459 | iter 2295 | Running Loss 0.018919690698385238\n",
      "epoch 460 | iter 2300 | Running Loss 0.017908512614667416\n",
      "epoch 461 | iter 2305 | Running Loss 0.01923651359975338\n",
      "epoch 462 | iter 2310 | Running Loss 0.019366573169827463\n",
      "epoch 463 | iter 2315 | Running Loss 0.018417272344231607\n",
      "epoch 464 | iter 2320 | Running Loss 0.019561774842441083\n",
      "epoch 465 | iter 2325 | Running Loss 0.019141189567744733\n",
      "epoch 466 | iter 2330 | Running Loss 0.01968744061887264\n",
      "epoch 467 | iter 2335 | Running Loss 0.01867128610610962\n",
      "epoch 468 | iter 2340 | Running Loss 0.0187147282063961\n",
      "epoch 469 | iter 2345 | Running Loss 0.020110999792814256\n",
      "epoch 470 | iter 2350 | Running Loss 0.017587948217988016\n",
      "epoch 471 | iter 2355 | Running Loss 0.02010632548481226\n",
      "epoch 472 | iter 2360 | Running Loss 0.018440261855721472\n",
      "epoch 473 | iter 2365 | Running Loss 0.01709576565772295\n",
      "epoch 474 | iter 2370 | Running Loss 0.01814659610390663\n",
      "epoch 475 | iter 2375 | Running Loss 0.019223884493112565\n",
      "epoch 476 | iter 2380 | Running Loss 0.022170288115739824\n",
      "epoch 477 | iter 2385 | Running Loss 0.016923611238598823\n",
      "epoch 478 | iter 2390 | Running Loss 0.018215441890060902\n",
      "epoch 479 | iter 2395 | Running Loss 0.01806888896971941\n",
      "epoch 480 | iter 2400 | Running Loss 0.02097213305532932\n",
      "epoch 481 | iter 2405 | Running Loss 0.017232819460332394\n",
      "epoch 482 | iter 2410 | Running Loss 0.018237230926752092\n",
      "epoch 483 | iter 2415 | Running Loss 0.018600411899387836\n",
      "epoch 484 | iter 2420 | Running Loss 0.01921252254396677\n",
      "epoch 485 | iter 2425 | Running Loss 0.019358987733721734\n",
      "epoch 486 | iter 2430 | Running Loss 0.018631233274936675\n",
      "epoch 487 | iter 2435 | Running Loss 0.0196608928963542\n",
      "epoch 488 | iter 2440 | Running Loss 0.01769942045211792\n",
      "epoch 489 | iter 2445 | Running Loss 0.018020187318325043\n",
      "epoch 490 | iter 2450 | Running Loss 0.019650955498218537\n",
      "epoch 491 | iter 2455 | Running Loss 0.017546032555401325\n",
      "epoch 492 | iter 2460 | Running Loss 0.01892180573195219\n",
      "epoch 493 | iter 2465 | Running Loss 0.017137390561401844\n",
      "epoch 494 | iter 2470 | Running Loss 0.019336927868425845\n",
      "epoch 495 | iter 2475 | Running Loss 0.01784327682107687\n",
      "epoch 496 | iter 2480 | Running Loss 0.019481414556503297\n",
      "epoch 497 | iter 2485 | Running Loss 0.017143051140010357\n",
      "epoch 498 | iter 2490 | Running Loss 0.017167769744992255\n",
      "epoch 499 | iter 2495 | Running Loss 0.01769310925155878\n"
     ]
    }
   ],
   "source": [
    "train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-481-3b3d9b4ea999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'torch.FloatTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcombined_zeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "torch_model.eval()\n",
    "i = random.randint(0, X_train.shape[0])\n",
    "example = X_train[i]\n",
    "example = torch.from_numpy(example.reshape(1, 250, 1)).type('torch.FloatTensor')\n",
    "pred = torch_model(example).detach().numpy()\n",
    "combined = np.concatenate([example[0].numpy(), y_train[i].T], axis=0).squeeze()\n",
    "combined_zeros = np.concatenate([np.zeros_like(example.squeeze()),pred]).squeeze()\n",
    "\n",
    "plt.plot(combined[-100:])\n",
    "plt.plot(combined_zeros[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250, 1])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import History, EarlyStopping, Callback\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/500\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 0.2761 - val_loss: 0.2136\n",
      "Epoch 2/500\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.2033 - val_loss: 0.1374\n",
      "Epoch 3/500\n",
      "240/240 [==============================] - 0s 83us/step - loss: 0.1303 - val_loss: 0.0901\n",
      "Epoch 4/500\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.1051 - val_loss: 0.0921\n",
      "Epoch 5/500\n",
      "240/240 [==============================] - 0s 93us/step - loss: 0.1026 - val_loss: 0.0849\n",
      "Epoch 6/500\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.0943 - val_loss: 0.0772\n",
      "Epoch 7/500\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0921 - val_loss: 0.0741\n",
      "Epoch 8/500\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0857 - val_loss: 0.0683\n",
      "Epoch 9/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0815 - val_loss: 0.0609\n",
      "Epoch 10/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0745 - val_loss: 0.0551\n",
      "Epoch 11/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0696 - val_loss: 0.0501\n",
      "Epoch 12/500\n",
      "240/240 [==============================] - 0s 114us/step - loss: 0.0624 - val_loss: 0.0469\n",
      "Epoch 13/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0579 - val_loss: 0.0435\n",
      "Epoch 14/500\n",
      "240/240 [==============================] - 0s 99us/step - loss: 0.0575 - val_loss: 0.0424\n",
      "Epoch 15/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0522 - val_loss: 0.0429\n",
      "Epoch 16/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0524 - val_loss: 0.0411\n",
      "Epoch 17/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0499 - val_loss: 0.0398\n",
      "Epoch 18/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0471 - val_loss: 0.0372\n",
      "Epoch 19/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0449 - val_loss: 0.0355\n",
      "Epoch 20/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0425 - val_loss: 0.0336\n",
      "Epoch 21/500\n",
      "240/240 [==============================] - 0s 99us/step - loss: 0.0437 - val_loss: 0.0328\n",
      "Epoch 22/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0452 - val_loss: 0.0322\n",
      "Epoch 23/500\n",
      "240/240 [==============================] - 0s 99us/step - loss: 0.0422 - val_loss: 0.0312\n",
      "Epoch 24/500\n",
      "240/240 [==============================] - 0s 98us/step - loss: 0.0385 - val_loss: 0.0305\n",
      "Epoch 25/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0388 - val_loss: 0.0293\n",
      "Epoch 26/500\n",
      "240/240 [==============================] - 0s 99us/step - loss: 0.0364 - val_loss: 0.0287\n",
      "Epoch 27/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0348 - val_loss: 0.0289\n",
      "Epoch 28/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0359 - val_loss: 0.0273\n",
      "Epoch 29/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0341 - val_loss: 0.0274\n",
      "Epoch 30/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0364 - val_loss: 0.0258\n",
      "Epoch 31/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0340 - val_loss: 0.0253\n",
      "Epoch 32/500\n",
      "240/240 [==============================] - 0s 98us/step - loss: 0.0342 - val_loss: 0.0265\n",
      "Epoch 33/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0352 - val_loss: 0.0252\n",
      "Epoch 34/500\n",
      "240/240 [==============================] - 0s 110us/step - loss: 0.0356 - val_loss: 0.0252\n",
      "Epoch 35/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0315 - val_loss: 0.0245\n",
      "Epoch 36/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0366 - val_loss: 0.0249\n",
      "Epoch 37/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0327 - val_loss: 0.0243\n",
      "Epoch 38/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0342 - val_loss: 0.0258\n",
      "Epoch 39/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0300 - val_loss: 0.0237\n",
      "Epoch 40/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0333 - val_loss: 0.0257\n",
      "Epoch 41/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0284 - val_loss: 0.0234\n",
      "Epoch 42/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0314 - val_loss: 0.0247\n",
      "Epoch 43/500\n",
      "240/240 [==============================] - 0s 110us/step - loss: 0.0297 - val_loss: 0.0231\n",
      "Epoch 44/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0302 - val_loss: 0.0239\n",
      "Epoch 45/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0305 - val_loss: 0.0234\n",
      "Epoch 46/500\n",
      "240/240 [==============================] - 0s 114us/step - loss: 0.0299 - val_loss: 0.0224\n",
      "Epoch 47/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0318 - val_loss: 0.0247\n",
      "Epoch 48/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0296 - val_loss: 0.0229\n",
      "Epoch 49/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0317 - val_loss: 0.0259\n",
      "Epoch 50/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0281 - val_loss: 0.0221\n",
      "Epoch 51/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0277 - val_loss: 0.0226\n",
      "Epoch 52/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0305 - val_loss: 0.0230\n",
      "Epoch 53/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0256 - val_loss: 0.0216\n",
      "Epoch 54/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0269 - val_loss: 0.0251\n",
      "Epoch 55/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0291 - val_loss: 0.0222\n",
      "Epoch 56/500\n",
      "240/240 [==============================] - 0s 116us/step - loss: 0.0260 - val_loss: 0.0217\n",
      "Epoch 57/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0264 - val_loss: 0.0243\n",
      "Epoch 58/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0243 - val_loss: 0.0220\n",
      "Epoch 59/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0253 - val_loss: 0.0254\n",
      "Epoch 60/500\n",
      "240/240 [==============================] - 0s 113us/step - loss: 0.0270 - val_loss: 0.0219\n",
      "Epoch 61/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0274 - val_loss: 0.0237\n",
      "Epoch 62/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0247 - val_loss: 0.0215\n",
      "Epoch 63/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0247 - val_loss: 0.0215\n",
      "Epoch 64/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0238 - val_loss: 0.0245\n",
      "Epoch 65/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0241 - val_loss: 0.0211\n",
      "Epoch 66/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0251 - val_loss: 0.0251\n",
      "Epoch 67/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0242 - val_loss: 0.0212\n",
      "Epoch 68/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0257 - val_loss: 0.0240\n",
      "Epoch 69/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0254 - val_loss: 0.0223\n",
      "Epoch 70/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0232 - val_loss: 0.0214\n",
      "Epoch 71/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0238 - val_loss: 0.0243\n",
      "Epoch 72/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0236 - val_loss: 0.0208\n",
      "Epoch 73/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0223 - val_loss: 0.0222\n",
      "Epoch 74/500\n",
      "240/240 [==============================] - 0s 98us/step - loss: 0.0235 - val_loss: 0.0208\n",
      "Epoch 75/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0236 - val_loss: 0.0222\n",
      "Epoch 76/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0221 - val_loss: 0.0210\n",
      "Epoch 77/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0230 - val_loss: 0.0222\n",
      "Epoch 78/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0216 - val_loss: 0.0206\n",
      "Epoch 79/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0240 - val_loss: 0.0216\n",
      "Epoch 80/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0207 - val_loss: 0.0212\n",
      "Epoch 81/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0226 - val_loss: 0.0215\n",
      "Epoch 82/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0211 - val_loss: 0.0215\n",
      "Epoch 83/500\n",
      "240/240 [==============================] - 0s 110us/step - loss: 0.0209 - val_loss: 0.0209\n",
      "Epoch 84/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0224 - val_loss: 0.0229\n",
      "Epoch 85/500\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0221 - val_loss: 0.0207\n",
      "Epoch 86/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0199 - val_loss: 0.0231\n",
      "Epoch 87/500\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0212 - val_loss: 0.0207\n",
      "Epoch 88/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0222 - val_loss: 0.0218\n",
      "Epoch 89/500\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0215 - val_loss: 0.0245\n",
      "Epoch 90/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0214 - val_loss: 0.0200\n",
      "Epoch 91/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0224 - val_loss: 0.0243\n",
      "Epoch 92/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0212 - val_loss: 0.0197\n",
      "Epoch 93/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0196 - val_loss: 0.0234\n",
      "Epoch 94/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0217 - val_loss: 0.0204\n",
      "Epoch 95/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0212 - val_loss: 0.0232\n",
      "Epoch 96/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0210 - val_loss: 0.0212\n",
      "Epoch 97/500\n",
      "240/240 [==============================] - 0s 113us/step - loss: 0.0204 - val_loss: 0.0219\n",
      "Epoch 98/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0205 - val_loss: 0.0224\n",
      "Epoch 99/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0196 - val_loss: 0.0201\n",
      "Epoch 100/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0190 - val_loss: 0.0234\n",
      "Epoch 101/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0200 - val_loss: 0.0199\n",
      "Epoch 102/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0195 - val_loss: 0.0209\n",
      "Epoch 103/500\n",
      "240/240 [==============================] - 0s 117us/step - loss: 0.0208 - val_loss: 0.0235\n",
      "Epoch 104/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0186 - val_loss: 0.0196\n",
      "Epoch 105/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0204 - val_loss: 0.0243\n",
      "Epoch 106/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0196 - val_loss: 0.0191\n",
      "Epoch 107/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0195 - val_loss: 0.0219\n",
      "Epoch 108/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0191 - val_loss: 0.0202\n",
      "Epoch 109/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0194 - val_loss: 0.0218\n",
      "Epoch 110/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0174 - val_loss: 0.0199\n",
      "Epoch 111/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0192 - val_loss: 0.0224\n",
      "Epoch 112/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0191 - val_loss: 0.0211\n",
      "Epoch 113/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0177 - val_loss: 0.0207\n",
      "Epoch 114/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0191 - val_loss: 0.0237\n",
      "Epoch 115/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0204 - val_loss: 0.0195\n",
      "Epoch 116/500\n",
      "240/240 [==============================] - 0s 114us/step - loss: 0.0200 - val_loss: 0.0245\n",
      "Epoch 117/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0195 - val_loss: 0.0191\n",
      "Epoch 118/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0194 - val_loss: 0.0223\n",
      "Epoch 119/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0190 - val_loss: 0.0194\n",
      "Epoch 120/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0186 - val_loss: 0.0219\n",
      "Epoch 121/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0196 - val_loss: 0.0205\n",
      "Epoch 122/500\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0176 - val_loss: 0.0205\n",
      "Epoch 123/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0171 - val_loss: 0.0207\n",
      "Epoch 124/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0186 - val_loss: 0.0212\n",
      "Epoch 125/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0174 - val_loss: 0.0205\n",
      "Epoch 126/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0169 - val_loss: 0.0207\n",
      "Epoch 127/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0183 - val_loss: 0.0215\n",
      "Epoch 128/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0171 - val_loss: 0.0204\n",
      "Epoch 129/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0178 - val_loss: 0.0211\n",
      "Epoch 130/500\n",
      "240/240 [==============================] - 0s 112us/step - loss: 0.0181 - val_loss: 0.0196\n",
      "Epoch 131/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0176 - val_loss: 0.0226\n",
      "Epoch 132/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0173 - val_loss: 0.0197\n",
      "Epoch 133/500\n",
      "240/240 [==============================] - 0s 96us/step - loss: 0.0177 - val_loss: 0.0198\n",
      "Epoch 134/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0167 - val_loss: 0.0205\n",
      "Epoch 135/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0172 - val_loss: 0.0249\n",
      "Epoch 136/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0172 - val_loss: 0.0195\n",
      "Epoch 137/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0164 - val_loss: 0.0240\n",
      "Epoch 138/500\n",
      "240/240 [==============================] - 0s 112us/step - loss: 0.0173 - val_loss: 0.0209\n",
      "Epoch 139/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0174 - val_loss: 0.0221\n",
      "Epoch 140/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0178 - val_loss: 0.0236\n",
      "Epoch 141/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0177 - val_loss: 0.0209\n",
      "Epoch 142/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0197 - val_loss: 0.0276\n",
      "Epoch 143/500\n",
      "240/240 [==============================] - 0s 116us/step - loss: 0.0197 - val_loss: 0.0204\n",
      "Epoch 144/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0180 - val_loss: 0.0234\n",
      "Epoch 145/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0165 - val_loss: 0.0196\n",
      "Epoch 146/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0168 - val_loss: 0.0203\n",
      "Epoch 147/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0154 - val_loss: 0.0209\n",
      "Epoch 148/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0175 - val_loss: 0.0201\n",
      "Epoch 149/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0160 - val_loss: 0.0238\n",
      "Epoch 150/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0151 - val_loss: 0.0195\n",
      "Epoch 151/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0144 - val_loss: 0.0236\n",
      "Epoch 152/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0149 - val_loss: 0.0199\n",
      "Epoch 153/500\n",
      "240/240 [==============================] - 0s 115us/step - loss: 0.0152 - val_loss: 0.0226\n",
      "Epoch 154/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0155 - val_loss: 0.0198\n",
      "Epoch 155/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0147 - val_loss: 0.0218\n",
      "Epoch 156/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0150 - val_loss: 0.0193\n",
      "Epoch 157/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0155 - val_loss: 0.0203\n",
      "Epoch 158/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0156 - val_loss: 0.0219\n",
      "Epoch 159/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0161 - val_loss: 0.0197\n",
      "Epoch 160/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0165 - val_loss: 0.0257\n",
      "Epoch 161/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0159 - val_loss: 0.0205\n",
      "Epoch 162/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0165 - val_loss: 0.0261\n",
      "Epoch 163/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0155 - val_loss: 0.0208\n",
      "Epoch 164/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0166 - val_loss: 0.0247\n",
      "Epoch 165/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0140 - val_loss: 0.0207\n",
      "Epoch 166/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0162 - val_loss: 0.0247\n",
      "Epoch 167/500\n",
      "240/240 [==============================] - 0s 97us/step - loss: 0.0169 - val_loss: 0.0204\n",
      "Epoch 168/500\n",
      "240/240 [==============================] - 0s 99us/step - loss: 0.0159 - val_loss: 0.0220\n",
      "Epoch 169/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0139 - val_loss: 0.0202\n",
      "Epoch 170/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0160 - val_loss: 0.0215\n",
      "Epoch 171/500\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0152 - val_loss: 0.0217\n",
      "Epoch 172/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0164 - val_loss: 0.0206\n",
      "Epoch 173/500\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0148 - val_loss: 0.0242\n",
      "Epoch 174/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0159 - val_loss: 0.0201\n",
      "Epoch 175/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0143 - val_loss: 0.0217\n",
      "Epoch 176/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0141 - val_loss: 0.0199\n",
      "Epoch 177/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0148 - val_loss: 0.0212\n",
      "Epoch 178/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0139 - val_loss: 0.0212\n",
      "Epoch 179/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0142 - val_loss: 0.0205\n",
      "Epoch 180/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0142 - val_loss: 0.0223\n",
      "Epoch 181/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0143 - val_loss: 0.0201\n",
      "Epoch 182/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0139 - val_loss: 0.0226\n",
      "Epoch 183/500\n",
      "240/240 [==============================] - 0s 97us/step - loss: 0.0131 - val_loss: 0.0209\n",
      "Epoch 184/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0143 - val_loss: 0.0215\n",
      "Epoch 185/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0128 - val_loss: 0.0215\n",
      "Epoch 186/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0143 - val_loss: 0.0207\n",
      "Epoch 187/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0134 - val_loss: 0.0231\n",
      "Epoch 188/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0136 - val_loss: 0.0210\n",
      "Epoch 189/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0133 - val_loss: 0.0222\n",
      "Epoch 190/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0133 - val_loss: 0.0215\n",
      "Epoch 191/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0137 - val_loss: 0.0212\n",
      "Epoch 192/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0139 - val_loss: 0.0212\n",
      "Epoch 193/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0139 - val_loss: 0.0217\n",
      "Epoch 194/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0129 - val_loss: 0.0214\n",
      "Epoch 195/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0129 - val_loss: 0.0224\n",
      "Epoch 196/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0132 - val_loss: 0.0214\n",
      "Epoch 197/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0137 - val_loss: 0.0206\n",
      "Epoch 198/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0139 - val_loss: 0.0240\n",
      "Epoch 199/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0137 - val_loss: 0.0199\n",
      "Epoch 200/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0142 - val_loss: 0.0239\n",
      "Epoch 201/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0137 - val_loss: 0.0201\n",
      "Epoch 202/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0136 - val_loss: 0.0219\n",
      "Epoch 203/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0132 - val_loss: 0.0215\n",
      "Epoch 204/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0138 - val_loss: 0.0215\n",
      "Epoch 205/500\n",
      "240/240 [==============================] - 0s 114us/step - loss: 0.0125 - val_loss: 0.0218\n",
      "Epoch 206/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0137 - val_loss: 0.0207\n",
      "Epoch 207/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0131 - val_loss: 0.0225\n",
      "Epoch 208/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0132 - val_loss: 0.0207\n",
      "Epoch 209/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0133 - val_loss: 0.0228\n",
      "Epoch 210/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0126 - val_loss: 0.0211\n",
      "Epoch 211/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0129 - val_loss: 0.0223\n",
      "Epoch 212/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0134 - val_loss: 0.0205\n",
      "Epoch 213/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0143 - val_loss: 0.0213\n",
      "Epoch 214/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0140 - val_loss: 0.0209\n",
      "Epoch 215/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0152 - val_loss: 0.0214\n",
      "Epoch 216/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0133 - val_loss: 0.0222\n",
      "Epoch 217/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0126 - val_loss: 0.0204\n",
      "Epoch 218/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0131 - val_loss: 0.0228\n",
      "Epoch 219/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0125 - val_loss: 0.0204\n",
      "Epoch 220/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0127 - val_loss: 0.0250\n",
      "Epoch 221/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0138 - val_loss: 0.0204\n",
      "Epoch 222/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0138 - val_loss: 0.0215\n",
      "Epoch 223/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0130 - val_loss: 0.0212\n",
      "Epoch 224/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0120 - val_loss: 0.0216\n",
      "Epoch 225/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0125 - val_loss: 0.0224\n",
      "Epoch 226/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0127 - val_loss: 0.0214\n",
      "Epoch 227/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0134 - val_loss: 0.0222\n",
      "Epoch 228/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0121 - val_loss: 0.0203\n",
      "Epoch 229/500\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0126 - val_loss: 0.0218\n",
      "Epoch 230/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0125 - val_loss: 0.0219\n",
      "Epoch 231/500\n",
      "240/240 [==============================] - 0s 113us/step - loss: 0.0127 - val_loss: 0.0209\n",
      "Epoch 232/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0118 - val_loss: 0.0223\n",
      "Epoch 233/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0117 - val_loss: 0.0211\n",
      "Epoch 234/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0124 - val_loss: 0.0238\n",
      "Epoch 235/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0130 - val_loss: 0.0208\n",
      "Epoch 236/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0126 - val_loss: 0.0234\n",
      "Epoch 237/500\n",
      "240/240 [==============================] - 0s 112us/step - loss: 0.0119 - val_loss: 0.0204\n",
      "Epoch 238/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0124 - val_loss: 0.0239\n",
      "Epoch 239/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0127 - val_loss: 0.0208\n",
      "Epoch 240/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0113 - val_loss: 0.0230\n",
      "Epoch 241/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0121 - val_loss: 0.0222\n",
      "Epoch 242/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0125 - val_loss: 0.0214\n",
      "Epoch 243/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0117 - val_loss: 0.0248\n",
      "Epoch 244/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0136 - val_loss: 0.0217\n",
      "Epoch 245/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0129 - val_loss: 0.0229\n",
      "Epoch 246/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0117 - val_loss: 0.0215\n",
      "Epoch 247/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0124 - val_loss: 0.0226\n",
      "Epoch 248/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0128 - val_loss: 0.0214\n",
      "Epoch 249/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0114 - val_loss: 0.0229\n",
      "Epoch 250/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0124 - val_loss: 0.0219\n",
      "Epoch 251/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0110 - val_loss: 0.0224\n",
      "Epoch 252/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0125 - val_loss: 0.0228\n",
      "Epoch 253/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0118 - val_loss: 0.0216\n",
      "Epoch 254/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0126 - val_loss: 0.0243\n",
      "Epoch 255/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0124 - val_loss: 0.0212\n",
      "Epoch 256/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0122 - val_loss: 0.0228\n",
      "Epoch 257/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0116 - val_loss: 0.0218\n",
      "Epoch 258/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0119 - val_loss: 0.0230\n",
      "Epoch 259/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0118 - val_loss: 0.0218\n",
      "Epoch 260/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0120 - val_loss: 0.0232\n",
      "Epoch 261/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0123 - val_loss: 0.0217\n",
      "Epoch 262/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0121 - val_loss: 0.0223\n",
      "Epoch 263/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0111 - val_loss: 0.0241\n",
      "Epoch 264/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0117 - val_loss: 0.0233\n",
      "Epoch 265/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0113 - val_loss: 0.0219\n",
      "Epoch 266/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0115 - val_loss: 0.0232\n",
      "Epoch 267/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0114 - val_loss: 0.0220\n",
      "Epoch 268/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0111 - val_loss: 0.0240\n",
      "Epoch 269/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0112 - val_loss: 0.0217\n",
      "Epoch 270/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0117 - val_loss: 0.0225\n",
      "Epoch 271/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0115 - val_loss: 0.0215\n",
      "Epoch 272/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0119 - val_loss: 0.0228\n",
      "Epoch 273/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0118 - val_loss: 0.0226\n",
      "Epoch 274/500\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0115 - val_loss: 0.0233\n",
      "Epoch 275/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0111 - val_loss: 0.0227\n",
      "Epoch 276/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0120 - val_loss: 0.0235\n",
      "Epoch 277/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0122 - val_loss: 0.0224\n",
      "Epoch 278/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0104 - val_loss: 0.0244\n",
      "Epoch 279/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0116 - val_loss: 0.0222\n",
      "Epoch 280/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0112 - val_loss: 0.0252\n",
      "Epoch 281/500\n",
      "240/240 [==============================] - 0s 117us/step - loss: 0.0111 - val_loss: 0.0219\n",
      "Epoch 282/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0107 - val_loss: 0.0224\n",
      "Epoch 283/500\n",
      "240/240 [==============================] - 0s 116us/step - loss: 0.0111 - val_loss: 0.0219\n",
      "Epoch 284/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0112 - val_loss: 0.0213\n",
      "Epoch 285/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0116 - val_loss: 0.0246\n",
      "Epoch 286/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0108 - val_loss: 0.0214\n",
      "Epoch 287/500\n",
      "240/240 [==============================] - 0s 110us/step - loss: 0.0119 - val_loss: 0.0236\n",
      "Epoch 288/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0113 - val_loss: 0.0226\n",
      "Epoch 289/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0115 - val_loss: 0.0220\n",
      "Epoch 290/500\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0101 - val_loss: 0.0223\n",
      "Epoch 291/500\n",
      "240/240 [==============================] - 0s 110us/step - loss: 0.0102 - val_loss: 0.0227\n",
      "Epoch 292/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0108 - val_loss: 0.0221\n",
      "Epoch 293/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0116 - val_loss: 0.0257\n",
      "Epoch 294/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0130 - val_loss: 0.0221\n",
      "Epoch 295/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0117 - val_loss: 0.0213\n",
      "Epoch 296/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0126 - val_loss: 0.0231\n",
      "Epoch 297/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0109 - val_loss: 0.0226\n",
      "Epoch 298/500\n",
      "240/240 [==============================] - 0s 114us/step - loss: 0.0102 - val_loss: 0.0225\n",
      "Epoch 299/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0108 - val_loss: 0.0243\n",
      "Epoch 300/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0106 - val_loss: 0.0232\n",
      "Epoch 301/500\n",
      "240/240 [==============================] - 0s 99us/step - loss: 0.0112 - val_loss: 0.0219\n",
      "Epoch 302/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0110 - val_loss: 0.0244\n",
      "Epoch 303/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0108 - val_loss: 0.0245\n",
      "Epoch 304/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0111 - val_loss: 0.0232\n",
      "Epoch 305/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0111 - val_loss: 0.0262\n",
      "Epoch 306/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0112 - val_loss: 0.0221\n",
      "Epoch 307/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0114 - val_loss: 0.0231\n",
      "Epoch 308/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0129 - val_loss: 0.0215\n",
      "Epoch 309/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0108 - val_loss: 0.0218\n",
      "Epoch 310/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0115 - val_loss: 0.0235\n",
      "Epoch 311/500\n",
      "240/240 [==============================] - 0s 112us/step - loss: 0.0113 - val_loss: 0.0218\n",
      "Epoch 312/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0109 - val_loss: 0.0227\n",
      "Epoch 313/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0106 - val_loss: 0.0227\n",
      "Epoch 314/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0100 - val_loss: 0.0219\n",
      "Epoch 315/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0107 - val_loss: 0.0244\n",
      "Epoch 316/500\n",
      "240/240 [==============================] - 0s 97us/step - loss: 0.0108 - val_loss: 0.0224\n",
      "Epoch 317/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0101 - val_loss: 0.0227\n",
      "Epoch 318/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0099 - val_loss: 0.0240\n",
      "Epoch 319/500\n",
      "240/240 [==============================] - 0s 99us/step - loss: 0.0104 - val_loss: 0.0224\n",
      "Epoch 320/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0099 - val_loss: 0.0229\n",
      "Epoch 321/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0099 - val_loss: 0.0222\n",
      "Epoch 322/500\n",
      "240/240 [==============================] - 0s 112us/step - loss: 0.0095 - val_loss: 0.0239\n",
      "Epoch 323/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0094 - val_loss: 0.0226\n",
      "Epoch 324/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0097 - val_loss: 0.0227\n",
      "Epoch 325/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0100 - val_loss: 0.0220\n",
      "Epoch 326/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0100 - val_loss: 0.0232\n",
      "Epoch 327/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0112 - val_loss: 0.0227\n",
      "Epoch 328/500\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0107 - val_loss: 0.0264\n",
      "Epoch 329/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0120 - val_loss: 0.0224\n",
      "Epoch 330/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0104 - val_loss: 0.0229\n",
      "Epoch 331/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0104 - val_loss: 0.0220\n",
      "Epoch 332/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0101 - val_loss: 0.0219\n",
      "Epoch 333/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0102 - val_loss: 0.0242\n",
      "Epoch 334/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0102 - val_loss: 0.0232\n",
      "Epoch 335/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0105 - val_loss: 0.0235\n",
      "Epoch 336/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0102 - val_loss: 0.0242\n",
      "Epoch 337/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0098 - val_loss: 0.0256\n",
      "Epoch 338/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0104 - val_loss: 0.0234\n",
      "Epoch 339/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0102 - val_loss: 0.0267\n",
      "Epoch 340/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0090 - val_loss: 0.0236\n",
      "Epoch 341/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0099 - val_loss: 0.0248\n",
      "Epoch 342/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0094 - val_loss: 0.0244\n",
      "Epoch 343/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0095 - val_loss: 0.0239\n",
      "Epoch 344/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0103 - val_loss: 0.0224\n",
      "Epoch 345/500\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0094 - val_loss: 0.0225\n",
      "Epoch 346/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0109 - val_loss: 0.0236\n",
      "Epoch 347/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0100 - val_loss: 0.0236\n",
      "Epoch 348/500\n",
      "240/240 [==============================] - 0s 115us/step - loss: 0.0099 - val_loss: 0.0250\n",
      "Epoch 349/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0099 - val_loss: 0.0228\n",
      "Epoch 350/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0103 - val_loss: 0.0231\n",
      "Epoch 351/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0108 - val_loss: 0.0251\n",
      "Epoch 352/500\n",
      "240/240 [==============================] - 0s 110us/step - loss: 0.0116 - val_loss: 0.0239\n",
      "Epoch 353/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0118 - val_loss: 0.0264\n",
      "Epoch 354/500\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0102 - val_loss: 0.0226\n",
      "Epoch 355/500\n",
      "240/240 [==============================] - 0s 99us/step - loss: 0.0108 - val_loss: 0.0247\n",
      "Epoch 356/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0098 - val_loss: 0.0231\n",
      "Epoch 357/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0109 - val_loss: 0.0265\n",
      "Epoch 358/500\n",
      "240/240 [==============================] - 0s 97us/step - loss: 0.0096 - val_loss: 0.0232\n",
      "Epoch 359/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0099 - val_loss: 0.0256\n",
      "Epoch 360/500\n",
      "240/240 [==============================] - 0s 112us/step - loss: 0.0097 - val_loss: 0.0248\n",
      "Epoch 361/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0099 - val_loss: 0.0237\n",
      "Epoch 362/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0108 - val_loss: 0.0266\n",
      "Epoch 363/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0113 - val_loss: 0.0227\n",
      "Epoch 364/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0105 - val_loss: 0.0246\n",
      "Epoch 365/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0095 - val_loss: 0.0233\n",
      "Epoch 366/500\n",
      "240/240 [==============================] - 0s 116us/step - loss: 0.0094 - val_loss: 0.0247\n",
      "Epoch 367/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0109 - val_loss: 0.0249\n",
      "Epoch 368/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0101 - val_loss: 0.0241\n",
      "Epoch 369/500\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0105 - val_loss: 0.0249\n",
      "Epoch 370/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0090 - val_loss: 0.0241\n",
      "Epoch 371/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0099 - val_loss: 0.0247\n",
      "Epoch 372/500\n",
      "240/240 [==============================] - 0s 118us/step - loss: 0.0092 - val_loss: 0.0245\n",
      "Epoch 373/500\n",
      "240/240 [==============================] - 0s 127us/step - loss: 0.0100 - val_loss: 0.0245\n",
      "Epoch 374/500\n",
      "240/240 [==============================] - 0s 94us/step - loss: 0.0087 - val_loss: 0.0240\n",
      "Epoch 375/500\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0095 - val_loss: 0.0253\n",
      "Epoch 376/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0093 - val_loss: 0.0252\n",
      "Epoch 377/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0094 - val_loss: 0.0239\n",
      "Epoch 378/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0106 - val_loss: 0.0245\n",
      "Epoch 379/500\n",
      "240/240 [==============================] - 0s 113us/step - loss: 0.0094 - val_loss: 0.0259\n",
      "Epoch 380/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0099 - val_loss: 0.0241\n",
      "Epoch 381/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0102 - val_loss: 0.0255\n",
      "Epoch 382/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0098 - val_loss: 0.0236\n",
      "Epoch 383/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0104 - val_loss: 0.0249\n",
      "Epoch 384/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0098 - val_loss: 0.0263\n",
      "Epoch 385/500\n",
      "240/240 [==============================] - 0s 121us/step - loss: 0.0108 - val_loss: 0.0222\n",
      "Epoch 386/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0094 - val_loss: 0.0251\n",
      "Epoch 387/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0098 - val_loss: 0.0226\n",
      "Epoch 388/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0091 - val_loss: 0.0244\n",
      "Epoch 389/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0092 - val_loss: 0.0245\n",
      "Epoch 390/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0090 - val_loss: 0.0243\n",
      "Epoch 391/500\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0095 - val_loss: 0.0234\n",
      "Epoch 392/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0095 - val_loss: 0.0250\n",
      "Epoch 393/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0090 - val_loss: 0.0246\n",
      "Epoch 394/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0086 - val_loss: 0.0243\n",
      "Epoch 395/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0085 - val_loss: 0.0249\n",
      "Epoch 396/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0095 - val_loss: 0.0238\n",
      "Epoch 397/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0089 - val_loss: 0.0257\n",
      "Epoch 398/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0094 - val_loss: 0.0238\n",
      "Epoch 399/500\n",
      "240/240 [==============================] - 0s 96us/step - loss: 0.0092 - val_loss: 0.0251\n",
      "Epoch 400/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0088 - val_loss: 0.0255\n",
      "Epoch 401/500\n",
      "240/240 [==============================] - 0s 118us/step - loss: 0.0092 - val_loss: 0.0243\n",
      "Epoch 402/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0087 - val_loss: 0.0235\n",
      "Epoch 403/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0087 - val_loss: 0.0246\n",
      "Epoch 404/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0092 - val_loss: 0.0244\n",
      "Epoch 405/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0095 - val_loss: 0.0248\n",
      "Epoch 406/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0090 - val_loss: 0.0243\n",
      "Epoch 407/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0094 - val_loss: 0.0260\n",
      "Epoch 408/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0097 - val_loss: 0.0258\n",
      "Epoch 409/500\n",
      "240/240 [==============================] - 0s 110us/step - loss: 0.0091 - val_loss: 0.0244\n",
      "Epoch 410/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0091 - val_loss: 0.0243\n",
      "Epoch 411/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0085 - val_loss: 0.0248\n",
      "Epoch 412/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0102 - val_loss: 0.0245\n",
      "Epoch 413/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0090 - val_loss: 0.0244\n",
      "Epoch 414/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0086 - val_loss: 0.0266\n",
      "Epoch 415/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0088 - val_loss: 0.0252\n",
      "Epoch 416/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0086 - val_loss: 0.0267\n",
      "Epoch 417/500\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0093 - val_loss: 0.0263\n",
      "Epoch 418/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0092 - val_loss: 0.0247\n",
      "Epoch 419/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0099 - val_loss: 0.0291\n",
      "Epoch 420/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0100 - val_loss: 0.0256\n",
      "Epoch 421/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0092 - val_loss: 0.0252\n",
      "Epoch 422/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0087 - val_loss: 0.0269\n",
      "Epoch 423/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0080 - val_loss: 0.0256\n",
      "Epoch 424/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0094 - val_loss: 0.0249\n",
      "Epoch 425/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0088 - val_loss: 0.0270\n",
      "Epoch 426/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0087 - val_loss: 0.0252\n",
      "Epoch 427/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0096 - val_loss: 0.0280\n",
      "Epoch 428/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0090 - val_loss: 0.0251\n",
      "Epoch 429/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0091 - val_loss: 0.0258\n",
      "Epoch 430/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0086 - val_loss: 0.0249\n",
      "Epoch 431/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0081 - val_loss: 0.0245\n",
      "Epoch 432/500\n",
      "240/240 [==============================] - 0s 113us/step - loss: 0.0091 - val_loss: 0.0269\n",
      "Epoch 433/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0088 - val_loss: 0.0263\n",
      "Epoch 434/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0089 - val_loss: 0.0265\n",
      "Epoch 435/500\n",
      "240/240 [==============================] - 0s 112us/step - loss: 0.0089 - val_loss: 0.0269\n",
      "Epoch 436/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0088 - val_loss: 0.0246\n",
      "Epoch 437/500\n",
      "240/240 [==============================] - 0s 118us/step - loss: 0.0088 - val_loss: 0.0258\n",
      "Epoch 438/500\n",
      "240/240 [==============================] - 0s 119us/step - loss: 0.0090 - val_loss: 0.0255\n",
      "Epoch 439/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0094 - val_loss: 0.0254\n",
      "Epoch 440/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0093 - val_loss: 0.0261\n",
      "Epoch 441/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0082 - val_loss: 0.0255\n",
      "Epoch 442/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0084 - val_loss: 0.0258\n",
      "Epoch 443/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0086 - val_loss: 0.0249\n",
      "Epoch 444/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0086 - val_loss: 0.0259\n",
      "Epoch 445/500\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0085 - val_loss: 0.0270\n",
      "Epoch 446/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0083 - val_loss: 0.0265\n",
      "Epoch 447/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0082 - val_loss: 0.0266\n",
      "Epoch 448/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0084 - val_loss: 0.0288\n",
      "Epoch 449/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0087 - val_loss: 0.0247\n",
      "Epoch 450/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0078 - val_loss: 0.0253\n",
      "Epoch 451/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0087 - val_loss: 0.0272\n",
      "Epoch 452/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0076 - val_loss: 0.0262\n",
      "Epoch 453/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0093 - val_loss: 0.0266\n",
      "Epoch 454/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0075 - val_loss: 0.0258\n",
      "Epoch 455/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0088 - val_loss: 0.0255\n",
      "Epoch 456/500\n",
      "240/240 [==============================] - 0s 99us/step - loss: 0.0085 - val_loss: 0.0260\n",
      "Epoch 457/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0084 - val_loss: 0.0265\n",
      "Epoch 458/500\n",
      "240/240 [==============================] - 0s 114us/step - loss: 0.0083 - val_loss: 0.0279\n",
      "Epoch 459/500\n",
      "240/240 [==============================] - 0s 113us/step - loss: 0.0092 - val_loss: 0.0263\n",
      "Epoch 460/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0079 - val_loss: 0.0258\n",
      "Epoch 461/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0088 - val_loss: 0.0276\n",
      "Epoch 462/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0089 - val_loss: 0.0253\n",
      "Epoch 463/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0086 - val_loss: 0.0255\n",
      "Epoch 464/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0081 - val_loss: 0.0297\n",
      "Epoch 465/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0100 - val_loss: 0.0247\n",
      "Epoch 466/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0090 - val_loss: 0.0258\n",
      "Epoch 467/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0084 - val_loss: 0.0280\n",
      "Epoch 468/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0080 - val_loss: 0.0259\n",
      "Epoch 469/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0087 - val_loss: 0.0272\n",
      "Epoch 470/500\n",
      "240/240 [==============================] - 0s 97us/step - loss: 0.0076 - val_loss: 0.0265\n",
      "Epoch 471/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0080 - val_loss: 0.0271\n",
      "Epoch 472/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0081 - val_loss: 0.0253\n",
      "Epoch 473/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0087 - val_loss: 0.0282\n",
      "Epoch 474/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0088 - val_loss: 0.0255\n",
      "Epoch 475/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0085 - val_loss: 0.0251\n",
      "Epoch 476/500\n",
      "240/240 [==============================] - 0s 110us/step - loss: 0.0084 - val_loss: 0.0269\n",
      "Epoch 477/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0088 - val_loss: 0.0241\n",
      "Epoch 478/500\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0079 - val_loss: 0.0250\n",
      "Epoch 479/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0077 - val_loss: 0.0245\n",
      "Epoch 480/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0079 - val_loss: 0.0249\n",
      "Epoch 481/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0082 - val_loss: 0.0265\n",
      "Epoch 482/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0083 - val_loss: 0.0256\n",
      "Epoch 483/500\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0083 - val_loss: 0.0270\n",
      "Epoch 484/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0091 - val_loss: 0.0305\n",
      "Epoch 485/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0096 - val_loss: 0.0255\n",
      "Epoch 486/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0090 - val_loss: 0.0288\n",
      "Epoch 487/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0088 - val_loss: 0.0263\n",
      "Epoch 488/500\n",
      "240/240 [==============================] - 0s 114us/step - loss: 0.0078 - val_loss: 0.0275\n",
      "Epoch 489/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0072 - val_loss: 0.0274\n",
      "Epoch 490/500\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0080 - val_loss: 0.0267\n",
      "Epoch 491/500\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0078 - val_loss: 0.0271\n",
      "Epoch 492/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0079 - val_loss: 0.0254\n",
      "Epoch 493/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0080 - val_loss: 0.0278\n",
      "Epoch 494/500\n",
      "240/240 [==============================] - 0s 99us/step - loss: 0.0083 - val_loss: 0.0261\n",
      "Epoch 495/500\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0085 - val_loss: 0.0274\n",
      "Epoch 496/500\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0083 - val_loss: 0.0278\n",
      "Epoch 497/500\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0080 - val_loss: 0.0274\n",
      "Epoch 498/500\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0077 - val_loss: 0.0263\n",
      "Epoch 499/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0075 - val_loss: 0.0267\n",
      "Epoch 500/500\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0083 - val_loss: 0.0271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f192064b748>"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(\n",
    "    80,\n",
    "    input_shape=(None, X_train.shape[2]),\n",
    "    return_sequences=True)\n",
    ")\n",
    "\n",
    "model.add(Dropout(.3))\n",
    "\n",
    "model.add(LSTM(\n",
    "    80,\n",
    "    return_sequences=False)\n",
    ")\n",
    "\n",
    "model.add(Dropout(.3))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "               optimizer='adam')\n",
    "\n",
    "model.fit(X_train,\n",
    "    y_train.squeeze(),\n",
    "    batch_size=64,\n",
    "    epochs=500,\n",
    "    validation_split=.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_activation = model = Sequential([Activation('linear')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If your data is in the form of symbolic tensors, you should specify the `steps` argument (instead of the `batch_size` argument, because symbolic tensors are expected to produce batches of input data).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-509-11ffbf445794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mne-4/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.conda/envs/mne-4/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    248\u001b[0m                                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                                     \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                                     steps_name='steps')\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m# Check if callbacks have not been already configured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mne-4/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_num_samples\u001b[0;34m(ins, batch_size, steps, steps_name)\u001b[0m\n\u001b[1;32m    569\u001b[0m             raise ValueError(\n\u001b[1;32m    570\u001b[0m                 \u001b[0;34m'If your data is in the form of symbolic tensors, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m                 \u001b[0;34m'you should specify the `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'` argument '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0;34m'(instead of the `batch_size` argument, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0;34m'because symbolic tensors are expected to produce '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: If your data is in the form of symbolic tensors, you should specify the `steps` argument (instead of the `batch_size` argument, because symbolic tensors are expected to produce batches of input data)."
     ]
    }
   ],
   "source": [
    "x = np.random.randn(4, 10, 20)\n",
    "a = linear_activation.predict(tf.convert_to_tensor(x)).numpy()\n",
    "print(np.sum(abs(x-a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try initializing with same variables\n",
    "# add pytest (read about it)\n",
    "# try porting to torch\n",
    "# try training keras model without fit function \n",
    "# https://stackoverflow.com/questions/56915567/keras-vs-pytorch-lstm-different-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchLSTM(\n",
      "  (lstm1): LSTM(1, 80, batch_first=True)\n",
      "  (dropout1): Dropout(p=0.3)\n",
      "  (lstm2): LSTM(80, 80, batch_first=True)\n",
      "  (dropout2): Dropout(p=0.3)\n",
      "  (fc1): Linear(in_features=80, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, None, 80)          105920    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, None, 80)          0         \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 80)                51520     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                810       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 158,250\n",
      "Trainable params: 158,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x7f192a9360f0>\n"
     ]
    }
   ],
   "source": [
    "print(model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f19123c4390>]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = random.randint(0, X_train.shape[0])\n",
    "example = X_train[i]\n",
    "pred = model.predict(example.reshape(1,1,-1))\n",
    "combined = np.concatenate([example,y_train[i]], axis=1).squeeze()\n",
    "combined_zeros = np.concatenate([np.zeros_like(example),pred], axis=1).squeeze()\n",
    "\n",
    "plt.plot(combined[-100:])\n",
    "plt.plot(combined_zeros[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model.eval()\n",
    "i = random.randint(0, X_train.shape[0])\n",
    "example = X_train[i]\n",
    "example = torch.from_numpy(example.reshape(1, 250, 1)).type('torch.FloatTensor')\n",
    "pred = torch_model(example).detach().numpy()\n",
    "combined = np.concatenate([example[0].numpy(), y_train[i].T], axis=0).squeeze()\n",
    "combined_zeros = np.concatenate([np.zeros_like(example),pred], axis=1).squeeze()\n",
    "\n",
    "plt.plot(combined[-50:])\n",
    "plt.plot(combined_zeros[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from dynaconf import settings\n",
    "import pandas as pd\n",
    "from progress.bar import ChargingBar\n",
    "import multiprocessing as mp\n",
    "\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def load_eeg_file(filename):\n",
    "    hdf = h5py.File(filename, \"r\")\n",
    "    rec = hdf[\"record-0\"]\n",
    "    signals = rec[\"signals\"]\n",
    "    specs = {\n",
    "        \"sample_frequency\": rec.attrs[\"sample_frequency\"],\n",
    "        \"number_channels\": rec.attrs[\"number_channels\"]\n",
    "    }\n",
    "    return signals, specs\n",
    "\n",
    "def get_recordings_df(csv_file, max_num_examples, print_results_info=True):\n",
    "        recordings_df = pd.read_csv(csv_file)\n",
    "        if print_results_info:\n",
    "            print(f\"Found {len(recordings_df)} recordings\")\n",
    "\n",
    "        if max_num_examples is not None:\n",
    "            recordings_df = recordings_df[:max_num_examples]\n",
    "            if print_results_info:\n",
    "                print(f\"By set limit only using {len(recordings_df)} recordings\")\n",
    "        \n",
    "        return recordings_df\n",
    "    \n",
    "def normalize(x):\n",
    "    x_range = x.max()-x.min()\n",
    "    if x_range == 0:\n",
    "        return torch.zeros_like(x)\n",
    "    return (x-x.min())/x_range\n",
    "\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, length, select_channels, max_num_examples=None, root_dir=None, transform=None, remove_short_recordings=True, filter_freq=False):\n",
    "\n",
    "        if root_dir is None:\n",
    "            self.root_dir = settings.DATASET_DIR\n",
    "        else:\n",
    "            self.root_dir = root_dir\n",
    "\n",
    "        self.recordings_df = get_recordings_df(csv_file, max_num_examples)\n",
    "        self.length = length\n",
    "        self.select_channels = select_channels\n",
    "        self.transform = transform\n",
    "        self.filter_freq = filter_freq\n",
    "\n",
    "        if remove_short_recordings:\n",
    "            self.remove_recordings_too_short()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.recordings_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, start_pos = self.recordings_df.iloc[idx, :2]\n",
    "        signals, _  = load_eeg_file(filename)\n",
    "\n",
    "        # only keep selected channels\n",
    "        signals = signals[self.select_channels]\n",
    "        # only output selected portion\n",
    "        signals = signals[:, start_pos: start_pos + self.length]\n",
    "        \n",
    "#         if self.filter_freq:\n",
    "#             signals = butter_lowpass_filter2(signals, cutoff=settings.LOW_PASS_FILTER_CUTOFF, fs=settings.FREQUENCY).copy()\n",
    "\n",
    "        signals = torch.from_numpy(signals).type('torch.FloatTensor')\n",
    "        if self.transform:\n",
    "            signals = self.transform(signals)\n",
    "\n",
    "        return signals\n",
    "\n",
    "    def recording_is_sufficient_length(self, idx):\n",
    "        # returns idx if insufficient otherwise None\n",
    "        cur_recording = self.__getitem__(idx)\n",
    "        if cur_recording.shape[1] != self.length:\n",
    "            return idx\n",
    "        return None\n",
    "    \n",
    "    def remove_recordings_too_short(self):\n",
    "\n",
    "        print(\"Removing recordings of insufficient length...\")\n",
    "        original_len = self.__len__()\n",
    "\n",
    "        pool = mp.Pool(mp.cpu_count())\n",
    "        remove_indices = pool.map(self.recording_is_sufficient_length, list(range(original_len)))\n",
    "        remove_indices = [i for i in remove_indices if i is not None] # remove all nones\n",
    "\n",
    "        self.recordings_df = self.recordings_df.drop(remove_indices)\n",
    "        print(f\"Removed {len(remove_indices)} of {original_len} recordings. There are now {self.__len__()} recordings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mne-4]",
   "language": "python",
   "name": "conda-env-mne-4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
